What is the time complexity of insertion sort in best, average, and worst cases?	Best: O(n) when array is already sorted<br>Average: O(n²)<br>Worst: O(n²) when array is reverse sorted<br>Space: O(1) auxiliary	Sorting Algorithms Medium
Describe the insertion sort algorithm and when it's most effective.	Works like sorting playing cards. For each element, find correct position in sorted portion and insert.<br>Most effective for: small arrays (n < 50), nearly sorted arrays, online algorithms (can sort as data arrives)	Sorting Algorithms Medium
What is the recurrence relation for merge sort and its solution?	T(n) = 2T(n/2) + Θ(n)<br>Solution: T(n) = Θ(n log n) by Master Theorem (Case 2)<br>Divide array in half, recursively sort, then merge in linear time	Sorting Algorithms Medium
What are the advantages and disadvantages of merge sort?	Advantages: Stable sort, guaranteed O(n log n), good for external sorting, parallelizable<br>Disadvantages: O(n) extra space, not in-place, slower than quicksort in practice for small arrays	Sorting Algorithms Medium
How does the BUILD-MAX-HEAP operation achieve O(n) time complexity?	Calls MAX-HEAPIFY on nodes n/2 down to 1. Most nodes are leaves (no work), higher levels do more work but fewer nodes.<br>Analysis: Σ(h=0 to lg n) ⌈n/2^(h+1)⌉ * O(h) = O(n)	Sorting Algorithms Hard
Compare heapsort with quicksort and merge sort.	Heapsort: O(n log n) guaranteed, O(1) space, not stable, poor cache performance<br>Quicksort: O(n log n) average, O(1) space, not stable, good cache performance<br>Merge sort: O(n log n) guaranteed, O(n) space, stable, good for external sorting	Sorting Algorithms Medium
Why is randomized quicksort's expected time O(n log n)?	Expected depth is O(log n) because random pivot likely creates balanced partitions.<br>E[T(n)] = E[partitioning cost] + E[recursive calls]<br>With high probability, each level processes O(n) elements, total O(n log n)	Sorting Algorithms Hard
What is the role of the PARTITION subroutine in quicksort?	Rearranges subarray A[p..r] so pivot element is in final position.<br>All elements ≤ pivot are to its left, all elements ≥ pivot are to its right.<br>Returns pivot's final index. Time: Θ(n), Space: O(1)	Sorting Algorithms Medium
Under what conditions can counting sort achieve O(n + k) time?	When input consists of integers in range [0, k] where k = O(n).<br>Uses auxiliary array of size k to count occurrences.<br>Stable: maintains relative order of equal elements<br>Not comparison-based, so bypasses Ω(n log n) lower bound	Sorting Algorithms Medium
How does radix sort achieve linear time for integers?	Sorts d-digit numbers by sorting on each digit (least to most significant).<br>Uses stable sorting (like counting sort) for each digit.<br>Time: O(d(n + k)) where d = number of digits, k = range of each digit<br>For fixed-width integers, d is constant → O(n)	Sorting Algorithms Medium
When is bucket sort most effective and what is its expected time?	Most effective when input is uniformly distributed over [0, 1).<br>Divides interval into n buckets, sort each bucket individually.<br>Expected time: O(n) when input is uniform<br>Worst case: O(n²) when all elements go to same bucket	Sorting Algorithms Medium
How can you implement a queue using two stacks?	Use two stacks: inbox and outbox.<br>Enqueue: push to inbox<br>Dequeue: if outbox empty, pop all from inbox to outbox; then pop from outbox<br>Amortized O(1) per operation because each element moved at most twice	Data Structures Medium
What are the advantages of doubly linked lists over singly linked lists?	Can traverse in both directions<br>O(1) deletion given node pointer (vs O(n) for singly linked)<br>Easier implementation of certain algorithms<br>Disadvantage: Extra space for previous pointers	Data Structures Medium
Compare chaining vs open addressing for collision resolution in hash tables.	Chaining: Store collisions in linked lists. Simple, handles high load factors well.<br>Open addressing: Store all elements in table using probing. Better cache performance, more sensitive to load factor.<br>Both achieve O(1) average time with good hash function	Data Structures Medium
What is the load factor in hash tables and why does it matter?	Load factor α = n/m (elements/table size)<br>Chaining: Expected chain length is α, operations take O(1 + α)<br>Open addressing: As α approaches 1, probe sequences get longer<br>Typical threshold: rehash when α > 0.7-0.75	Data Structures Medium
What property makes binary search tree operations efficient?	BST property: for any node x, all nodes in left subtree ≤ x.key ≤ all nodes in right subtree<br>Allows elimination of half the search space at each step<br>Inorder traversal visits keys in sorted order<br>Height determines operation complexity	Data Structures Medium
How do you delete a node with two children from a BST?	Replace node with its inorder successor (or predecessor)<br>1. Find successor: leftmost node in right subtree<br>2. Copy successor's key to node being deleted<br>3. Delete successor (which has at most one child)<br>Maintains BST property	Data Structures Medium
Why do red-black tree properties guarantee O(log n) height?	Key insight: longest path (root to leaf) has ≤ 2× nodes as shortest path<br>Property 4 (red nodes have black children) means no two consecutive red nodes<br>Property 5 (same black height) means shortest path is all black<br>Therefore height ≤ 2 log(n+1)	Data Structures Hard
When do you need to recolor vs rotate in red-black tree insertion?	After inserting red node, if parent is also red:<br>Case 1: Uncle is red → recolor parent, uncle to black, grandparent to red<br>Case 2/3: Uncle is black → rotate to balance tree<br>Continue up tree until root or no red-red violation	Data Structures Hard
What are the three steps of the divide-and-conquer paradigm?	1. Divide: Split problem into smaller subproblems of same type<br>2. Conquer: Solve subproblems recursively (base case for small problems)<br>3. Combine: Merge solutions to get solution for original problem	Divide and Conquer Medium
How do you find the maximum subarray sum using divide and conquer?	Divide array at midpoint. Maximum subarray is either:<br>1. Entirely in left half (recurse)<br>2. Entirely in right half (recurse)<br>3. Crosses midpoint (find max left sum + max right sum)<br>Time: T(n) = 2T(n/2) + Θ(n) = Θ(n log n)	Divide and Conquer Medium
Describe Strassen's algorithm for matrix multiplication.	Reduces 2×2 matrix multiplication from 8 to 7 multiplications using clever additions.<br>Recursively applied: T(n) = 7T(n/2) + Θ(n²) = Θ(n^log₂7) ≈ Θ(n^2.807)<br>Better than standard Θ(n³) for large matrices, but high constant factors	Divide and Conquer Hard
What is the closest pair problem and how is it solved with divide and conquer?	Find two points with minimum Euclidean distance among n points.<br>Divide by vertical line, solve recursively for left and right halves.<br>Key insight: only check points within δ of dividing line (δ = min of left, right distances)<br>Time: O(n log n)	Divide and Conquer Medium
What conditions must a problem satisfy to apply dynamic programming?	1. Optimal Substructure: Optimal solution contains optimal solutions to subproblems<br>2. Overlapping Subproblems: Same subproblems appear multiple times in naive recursion<br>Without condition 2, divide-and-conquer is more appropriate	Dynamic Programming Medium
Compare top-down (memoization) vs bottom-up dynamic programming.	Top-down: Recursive with memoization, solves only needed subproblems, easier to code<br>Bottom-up: Iterative, solves all subproblems, better space efficiency, avoids recursion overhead<br>Both have same asymptotic complexity but different constants	Dynamic Programming Medium
What is the optimal substructure property in the activity selection problem?	If activity k is in optimal solution for activities i through j, then:<br>- Activities before k form optimal solution for activities i through k-1<br>- Activities after k form optimal solution for activities k+1 through j<br>Greedy choice: select earliest finishing activity	Dynamic Programming Medium
How do you reconstruct the actual solution from a DP table?	Store additional information during DP computation:<br>1. Keep parent pointers showing which choice led to optimal value<br>2. Or store the actual choices made at each step<br>3. Trace back from final answer to reconstruct sequence<br>Adds O(1) time and space per subproblem	Dynamic Programming Medium
What is the difference between longest common subsequence and longest common substring?	LCS: Characters can be non-contiguous. DP recurrence based on matching/skipping characters.<br>LCS substring: Characters must be contiguous. Can use rolling hash or suffix arrays.<br>LCS is more general and commonly appears in diff algorithms, bioinformatics	Dynamic Programming Medium
Describe the coin change problem and its DP solution.	Given coin denominations and amount, find minimum coins needed.<br>DP[i] = minimum coins for amount i<br>DP[i] = min(DP[i-coin] + 1) for all coins ≤ i<br>Time: O(amount × coins), Space: O(amount)	Dynamic Programming Medium
What is the travelling salesman problem and why is DP exponential?	Find shortest route visiting all cities exactly once.<br>DP state: (current city, set of visited cities)<br>DP[mask][i] = shortest path ending at city i having visited cities in mask<br>Time: O(n² × 2ⁿ), Space: O(n × 2ⁿ) - exponential in number of cities	Dynamic Programming Hard
What is the greedy choice property and how does it differ from optimal substructure?	Greedy choice: Locally optimal choice leads to globally optimal solution<br>Optimal substructure: Optimal solution contains optimal solutions to subproblems<br>Greedy makes choice before solving subproblems; DP considers all possibilities	Greedy Algorithms Medium
Why does the greedy algorithm work for the fractional knapsack problem?	Can take fractions of items, so always take highest value-to-weight ratio first.<br>Greedy choice: if item has highest ratio, taking maximum possible amount is optimal<br>Proof: Any optimal solution can be modified to take more of highest-ratio item first	Greedy Algorithms Medium
Describe Huffman coding and why it produces optimal prefix codes.	Bottom-up construction: merge two lowest-frequency nodes repeatedly<br>Greedy choice: merge lowest frequencies at each step<br>Optimal because any optimal code can be transformed to have same structure<br>Expected code length equals entropy (information theory)	Greedy Algorithms Medium
What is the cut property in minimum spanning trees?	For any cut (S, V-S) of graph, the minimum weight edge crossing the cut is in some MST<br>Prim's algorithm: grow tree by adding minimum weight edge from tree to non-tree vertices<br>Kruskal's algorithm: add minimum weight edges that don't create cycles	Greedy Algorithms Medium
Why doesn't the greedy algorithm work for the 0-1 knapsack problem?	Cannot take fractions, so highest value-to-weight ratio might not be optimal<br>Counterexample: capacity 10, items (weight, value): (5, 10), (4, 7), (6, 9)<br>Greedy by ratio takes (4,7) + (5,10) = 17, but optimal is (6,9) + (4,7) = 16... wait, that's wrong.<br>Actually: optimal is (5,10) + (4,7) = 17. Need better example where greedy fails.	Greedy Algorithms Medium
Compare adjacency matrix vs adjacency list representations of graphs.	Adjacency Matrix: Θ(V²) space, O(1) edge queries, Θ(V) to find all neighbors<br>Adjacency List: Θ(V + E) space, O(degree) edge queries, O(degree) to find neighbors<br>Matrix better for dense graphs, list better for sparse graphs	Graph Algorithms Medium
What tree property does BFS create and what problems does it solve?	Creates shortest-path tree from source (minimum number of edges)<br>Applications: shortest path in unweighted graphs, level-order traversal, bipartiteness testing<br>Time: O(V + E), Space: O(V) for queue and visited array	Graph Algorithms Medium
What are the different types of edges in a DFS forest?	Tree edges: part of DFS forest<br>Back edges: to ancestors (indicate cycles in undirected graphs)<br>Forward edges: to descendants (only in directed graphs)<br>Cross edges: between different subtrees<br>Classification helps with cycle detection, topological sorting	Graph Algorithms Medium
How does DFS-based topological sort work and when does it fail?	Perform DFS, add vertices to front of list when finished (decreasing finish time)<br>Works because if (u,v) is edge, then finish[u] > finish[v]<br>Fails if back edge found (indicates cycle in directed graph)<br>Time: O(V + E)	Graph Algorithms Medium
Describe the two-pass algorithm for finding strongly connected components.	1. Run DFS on G, compute finishing times<br>2. Run DFS on Gᵀ (transpose) in decreasing order of finishing times<br>3. Each DFS tree in pass 2 is one SCC<br>Why it works: finishing times respect SCC structure in transpose graph	Graph Algorithms Hard
What is the relaxation operation in shortest path algorithms?	For edge (u,v) with weight w:<br>if d[v] > d[u] + w:<br>    d[v] = d[u] + w<br>    π[v] = u<br>Improves estimate if better path found through u<br>All shortest-path algorithms use relaxation	Graph Algorithms Medium
Why does Dijkstra's algorithm fail with negative edge weights?	Greedy choice assumes shortest path to vertex won't improve once selected<br>Negative edges can create shorter paths through unprocessed vertices<br>Example: A→B (1), A→C (4), C→B (-3). Dijkstra picks B first but A→C→B is shorter<br>Bellman-Ford handles negative weights by relaxing all edges V-1 times	Graph Algorithms Medium
How does Bellman-Ford detect negative-weight cycles?	After V-1 iterations of relaxing all edges, run one more iteration<br>If any distance improves, negative cycle exists<br>Why: shortest simple path has ≤ V-1 edges, so distances shouldn't improve after V-1 iterations<br>Time: O(VE)	Graph Algorithms Medium
Compare the approaches of Floyd-Warshall vs Johnson's algorithm.	Floyd-Warshall: O(V³), works with negative edges, simple DP<br>Johnson's: O(V² log V + VE), better for sparse graphs, uses reweighting + Dijkstra<br>Johnson's reweighting: add vertex s connected to all vertices with weight 0, run Bellman-Ford from s	Graph Algorithms Hard
State the max-flow min-cut theorem.	The maximum flow from source to sink equals the minimum capacity of all cuts separating source and sink.<br>Any max flow and any min cut satisfy: flow value = cut capacity<br>Ford-Fulkerson finds max flow by finding augmenting paths until none exist	Network Flows Medium
What is an augmenting path and how does it improve flow?	Path from source to sink in residual graph with positive capacity<br>Residual capacity = min(residual capacities along path)<br>Augment: increase flow along forward edges, decrease along backward edges<br>Backward edges allow 'undoing' previous flow decisions	Network Flows Medium
Compare Ford-Fulkerson with Edmonds-Karp algorithm.	Ford-Fulkerson: Generic method, any augmenting path, O(E × |f*|) where f* is max flow<br>Edmonds-Karp: Uses BFS for shortest augmenting paths, O(VE²)<br>BFS ensures number of augmentations is O(VE), each taking O(E) time	Network Flows Medium
How do you model bipartite matching as a max flow problem?	Create source connected to all left vertices (capacity 1)<br>Create sink connected to all right vertices (capacity 1)<br>Original edges have capacity 1<br>Max flow = maximum matching size<br>Integer capacities guarantee integer max flow	Network Flows Medium
How does the KMP algorithm achieve linear time string matching?	Precomputes failure function: longest proper prefix that's also suffix<br>On mismatch, shifts pattern based on failure function instead of starting over<br>Never moves text pointer backward, each character examined at most twice<br>Time: O(n + m) total	String Algorithms Medium
What information does the failure function in KMP provide?	π[q] = length of longest proper prefix of P[1..q] that's also suffix<br>When mismatch at position q+1, can shift to position π[q]+1<br>Precomputation uses similar logic to pattern matching itself<br>Allows skipping redundant comparisons	String Algorithms Medium
How does the Rabin-Karp algorithm handle hash collisions?	Uses rolling hash for efficiency: remove leftmost character, add rightmost<br>Spurious hits: hash matches but strings differ<br>Must verify match character by character when hashes match<br>Expected time O(n + m), worst case O(nm) with many collisions	String Algorithms Medium
What makes suffix trees powerful for string processing?	Stores all suffixes in compressed trie, linear space after compression<br>Applications: substring search O(m), longest common substring, longest repeated substring<br>Construction: O(n) with Ukkonen's algorithm (complex)<br>Suffix arrays are simpler alternative with similar capabilities	String Algorithms Hard
What is Euler's theorem and how is it used in RSA?	If gcd(a,n) = 1, then a^φ(n) ≡ 1 (mod n) where φ(n) is Euler's totient function<br>RSA: choose p,q prime, n=pq, φ(n)=(p-1)(q-1)<br>Public key e, private key d where ed ≡ 1 (mod φ(n))<br>Decryption works because (m^e)^d ≡ m (mod n)	Number-Theoretic Algorithms Hard
How does the Miller-Rabin primality test work?	Based on: if n is prime and a^(n-1) ≡ 1 (mod n), then either a^d ≡ 1 (mod n) or a^(2^r × d) ≡ -1 (mod n) for some r<br>Write n-1 = 2^s × d where d is odd<br>Test multiple random witnesses, each has ≤ 1/4 probability of fooling composite n<br>k rounds give error probability ≤ (1/4)^k	Number-Theoretic Algorithms Hard
What are the possible outcomes when solving a linear program?	1. Optimal solution: finite optimum value achieved at vertex of feasible region<br>2. Unbounded: objective function can be made arbitrarily large/small<br>3. Infeasible: no point satisfies all constraints<br>Simplex algorithm moves between vertices to find optimum	Linear Programming Medium
How do you determine if a point is inside a convex polygon?	Method 1: Check if point is on same side of all edges (using cross products)<br>Method 2: Sum angles from point to vertices; inside if sum = 2π<br>Method 3: Ray casting (works for non-convex too): count intersections with polygon boundary<br>Time: O(n) for n vertices	Computational Geometry Medium
What problem does a disjoint-set data structure solve efficiently?	Maintains collection of disjoint dynamic sets, supports:<br>MAKE-SET(x): create singleton set<br>UNION(x,y): merge sets containing x and y<br>FIND-SET(x): return representative of set containing x<br>With path compression and union by rank: amortized O(α(n)) per operation	Advanced Data Structures Medium
How does a Fibonacci heap achieve O(1) amortized decrease-key?	Lazy approach: don't fix heap violations immediately<br>Decrease-key: just decrease value, cut if violates heap property<br>Consolidation happens during extract-min<br>Potential function accounts for trees and marked nodes<br>Key insight: each node loses at most 2 children before being cut	Advanced Data Structures Hard
What is the current best known complexity for matrix multiplication?	Current best is approximately O(n^2.373) by Le Gall (2014)<br>Strassen's algorithm: O(n^2.807)<br>Naive algorithm: O(n^3)<br>Conjecture: optimal is O(n^2) but not yet proven<br>Practical algorithms often use hybrid approaches	Matrix Operations Hard
How does the Fast Fourier Transform achieve O(n log n) polynomial multiplication?	Convert to point-value representation using roots of unity<br>Multiply point-values in O(n) time<br>Convert back using inverse FFT<br>Key insight: evaluation at 2n-th roots of unity can be done recursively<br>Divide-and-conquer: T(n) = 2T(n/2) + O(n) = O(n log n)	Polynomials and FFT Hard
What is the difference between Prim's and Kruskal's MST algorithms?	Prim's: Grow tree from arbitrary vertex, always add cheapest edge from tree to non-tree<br>Kruskal's: Sort all edges, add cheapest edges that don't create cycles<br>Prim's: O(E log V) with binary heap, O(E + V log V) with Fibonacci heap<br>Kruskal's: O(E log E) for sorting, uses Union-Find for cycle detection	Graph Algorithms Medium
How do you find bridges in an undirected graph?	Bridge: edge whose removal increases number of connected components<br>Use DFS with discovery times and low values<br>low[v] = minimum discovery time reachable from subtree rooted at v<br>Edge (u,v) is bridge if low[v] > disc[u] (v can't reach ancestor of u)<br>Time: O(V + E)	Graph Algorithms Hard
What is the articulation point (cut vertex) problem?	Articulation point: vertex whose removal increases connected components<br>Root is articulation point if it has > 1 child in DFS tree<br>Non-root v is articulation point if some child u has low[u] ≥ disc[v]<br>Means removing v disconnects u from ancestors of v<br>Time: O(V + E) with DFS	Graph Algorithms Hard
What is the matrix chain multiplication problem?	Find optimal parenthesization to minimize scalar multiplications<br>DP[i][j] = minimum multiplications for matrices Ai through Aj<br>DP[i][j] = min(DP[i][k] + DP[k+1][j] + pi-1*pk*pj) for k from i to j-1<br>Time: O(n³), Space: O(n²). Classic example of optimal substructure	Dynamic Programming Medium
How do you solve the palindrome partitioning problem with DP?	Find minimum cuts to partition string into palindromes<br>DP[i] = minimum cuts for substring s[0..i]<br>DP[i] = min(DP[j-1] + 1) for all j where s[j..i] is palindrome<br>Precompute palindrome table: P[i][j] = true if s[i..j] is palindrome<br>Time: O(n²)	Dynamic Programming Medium
What is the optimal binary search tree problem?	Given keys with search frequencies, build BST minimizing expected search cost<br>DP[i][j] = minimum cost for keys ki through kj<br>DP[i][j] = DP[i][r-1] + DP[r+1][j] + sum of frequencies from ki to kj<br>Try each key kr as root, take minimum<br>Time: O(n³)	Dynamic Programming Hard
How does a Fenwick Tree (Binary Indexed Tree) work?	Compact representation for prefix sums with O(log n) updates<br>Index i stores sum of elements from (i - lowbit(i) + 1) to i<br>lowbit(i) = i & (-i) gives rightmost set bit<br>Query: sum bits by removing lowbit, Update: add lowbit and propagate<br>Space: O(n), both operations O(log n)	Advanced Data Structures Medium
What operations does a segment tree support efficiently?	Range queries (sum, min, max) and point/range updates in O(log n)<br>Complete binary tree with 4n nodes for array of size n<br>Lazy propagation allows range updates without updating all nodes immediately<br>Applications: range minimum query, range sum with updates	Advanced Data Structures Medium
How does a trie (prefix tree) optimize string operations?	Tree where each path from root represents a string prefix<br>Shared prefixes stored only once, saves space<br>Insert/Search/Delete: O(length of string)<br>Applications: autocomplete, spell checker, IP routing<br>Space: O(ALPHABET_SIZE × N × M) worst case	Advanced Data Structures Medium
What is external sorting and how is merge sort adapted for it?	Sorting data that doesn't fit in memory, stored on external storage<br>K-way merge: divide data into sorted runs, merge k runs at a time<br>Minimizes I/O operations which dominate cost<br>Optimal k depends on available memory and I/O characteristics<br>Time complexity includes I/O cost	Sorting Algorithms Medium
How does shell sort work and what is its complexity?	Generalization of insertion sort using gap sequence<br>Sort elements gap positions apart, gradually reduce gap to 1<br>Final pass with gap=1 is regular insertion sort<br>Complexity depends on gap sequence: O(n^3/2) for some sequences<br>Practical algorithm, simple implementation	Sorting Algorithms Medium
What is the work-span model for analyzing parallel algorithms?	Work (T₁): total operations in sequential execution<br>Span (T∞): longest dependency path (critical path)<br>Speedup ≤ min(T₁/T∞, P) on P processors<br>Parallelism = T₁/T∞, ideal parallelism when T₁/T∞ >> P	Multithreaded Algorithms Medium
How do you parallelize merge sort efficiently?	Spawn parallel recursive calls for left and right halves<br>Sync before merging (merge itself can also be parallelized)<br>Work: O(n log n), Span: O(log³ n) with parallel merge<br>Parallelism: O(n log n / log³ n) = O(n / log² n)	Multithreaded Algorithms Medium
What is a van Emde Boas tree and what does it optimize?	Data structure for integers in universe {0, 1, ..., u-1}<br>Supports insert, delete, search, predecessor, successor in O(log log u)<br>Recursive structure: √u clusters of size √u each<br>Space: O(u) but often too much for practical use<br>Theoretically optimal for integer operations	Advanced Data Structures Hard
How does consistent hashing solve the load balancing problem?	Maps both keys and servers to points on a circle<br>Key assigned to next server clockwise on circle<br>Adding/removing servers affects only adjacent keys, not all keys<br>Achieves good load distribution with minimal remapping<br>Used in distributed systems like Amazon Dynamo	Advanced Data Structures Medium
What is a suffix array and how is it constructed efficiently?	Sorted array of all suffixes of a string<br>SA[i] = starting position of i-th smallest suffix<br>Naive: O(n² log n), DC3 algorithm: O(n)<br>With LCP array, supports many string operations efficiently<br>More space-efficient than suffix trees in practice	String Algorithms Hard
How does the Aho-Corasick algorithm find multiple patterns simultaneously?	Builds trie of all patterns, adds failure links for mismatches<br>Failure function similar to KMP but for multiple patterns<br>Processes text in single pass, reporting all pattern occurrences<br>Time: O(n + m + z) where n=text length, m=total pattern length, z=matches<br>Used in antivirus software, intrusion detection	String Algorithms Hard
What is a polynomial-time reduction and why is it important?	Problem A reduces to B if solution to B can solve A in polynomial time<br>If A reduces to B and B ∈ P, then A ∈ P<br>Used to prove NP-completeness: reduce known NP-complete problem to new problem<br>Transitivity of reductions allows building chains of NP-complete problems	Advanced Design Techniques Medium
What is the difference between NP-hard and NP-complete?	NP-hard: at least as hard as any problem in NP (includes harder problems)<br>NP-complete: in NP and NP-hard (hardest problems in NP)<br>All NP-complete problems reduce to each other<br>If any NP-complete problem ∈ P, then P = NP	Advanced Design Techniques Medium
How does the 2-approximation algorithm for vertex cover work?	Repeatedly find edge (u,v), add both u and v to cover, remove all edges incident to u or v<br>Analysis: each chosen edge must have at least one endpoint in optimal cover<br>We choose both endpoints, so |solution| ≤ 2|optimal|<br>Simple greedy algorithm with guaranteed performance bound	Advanced Design Techniques Medium
What is the traveling salesman problem and what approximation is possible?	Find shortest tour visiting all cities exactly once<br>General TSP: no constant approximation unless P=NP<br>Metric TSP (triangle inequality): 2-approximation using MST<br>Christofides algorithm: 3/2-approximation using minimum perfect matching<br>Held-Karp: exact algorithm in O(n²2ⁿ) time	Advanced Design Techniques Hard
How do you solve the maximum bipartite matching problem?	Model as max flow: source to left vertices, right vertices to sink, all capacities 1<br>Alternatively, use augmenting paths directly in bipartite graph<br>Hopcroft-Karp algorithm: O(E√V) using shortest augmenting paths<br>König's theorem: max matching = min vertex cover in bipartite graphs	Network Flows Medium
What is the minimum cost flow problem?	Find flow of given value with minimum cost<br>Generalizes shortest path (flow=1) and max flow (cost=0)<br>Can model as linear program or use specialized algorithms<br>Successive shortest path: augment along minimum cost paths<br>Applications: transportation, assignment problems	Network Flows Hard
How do you compute the convex hull using Graham's scan?	Sort points by polar angle with respect to bottom-most point<br>Maintain stack of hull vertices, remove points that make right turns<br>Uses cross product to determine turn direction<br>Time: O(n log n) for sorting, O(n) for scanning<br>Handles collinear points with special care	Computational Geometry Medium
What is the line sweep technique in computational geometry?	Process geometric objects by sweeping line across plane<br>Maintain data structure of active objects intersecting sweep line<br>Handle events when sweep line encounters object boundaries<br>Applications: segment intersection, Voronoi diagrams, closest pair<br>Reduces 2D problem to sequence of 1D problems	Computational Geometry Medium
How does the Boyer-Moore algorithm skip characters during string matching?	Uses bad character rule and good suffix rule to skip positions<br>Bad character: when mismatch occurs, skip to align character in pattern<br>Good suffix: use information about matched suffix to determine skip<br>Worst case O(nm) but often much faster in practice, especially for large alphabets<br>Preprocessing time: O(m + σ) where σ is alphabet size	String Algorithms Medium
What is amortized analysis and when is it useful?	Analyzes average cost per operation over sequence of operations<br>Useful when individual operations have varying costs but average is better<br>Three methods: aggregate, accounting, potential<br>Examples: dynamic arrays, splay trees, Fibonacci heaps<br>Gives tighter bounds than worst-case analysis of individual operations	Advanced Design Techniques Medium
How does the accounting method work in amortized analysis?	Assign amortized cost to each operation (may differ from actual cost)<br>Maintain credit balance: amortized - actual cost<br>Credit must never go negative (prepayment principle)<br>Total amortized cost is upper bound on total actual cost<br>Example: dynamic array doubling with amortized O(1) insertion	Advanced Design Techniques Medium
What is the difference between Las Vegas and Monte Carlo algorithms?	Las Vegas: randomized running time, always correct (e.g., randomized quicksort)<br>Monte Carlo: fixed running time, possibly incorrect with small probability<br>One-sided error: may say 'yes' when answer is 'no' (or vice versa)<br>Two-sided error: may err in either direction<br>Can convert between types in some cases	Advanced Design Techniques Medium
How does randomized quickselect achieve linear expected time?	Find k-th smallest element using random pivot<br>Partition around pivot, recurse on side containing k-th element<br>Expected size of recursive call is 3n/4 (with high probability)<br>T(n) = T(3n/4) + O(n) = O(n) by geometric series<br>Worst case still O(n²) but very unlikely	Advanced Design Techniques Medium
What is universal hashing and why is it important?	Family of hash functions where any two keys have small collision probability<br>For family H and keys x ≠ y: Pr[h(x) = h(y)] ≤ 1/m for random h ∈ H<br>Ensures good expected performance regardless of input<br>Example: h(x) = ((ax + b) mod p) mod m where p is prime, a,b random<br>Defends against adversarial inputs	Advanced Data Structures Hard
How does cuckoo hashing achieve O(1) worst-case lookup time?	Uses two hash tables and two hash functions<br>Each key stored in exactly one location: T1[h1(x)] or T2[h2(x)]<br>Insertion may require moving existing keys (cuckoo eviction)<br>Worst-case O(1) lookup, amortized O(1) insertion<br>Rehash entire table if insertion fails after too many evictions	Advanced Data Structures Hard
What is the set cover problem and how well can it be approximated?	Given universe U and collection S of subsets, find minimum subcollection covering U<br>Greedy algorithm: repeatedly choose set covering most uncovered elements<br>Approximation ratio: H_n ≈ ln n where n = |U|<br>This is optimal unless P = NP (ln n lower bound)<br>Applicationates: facility location, resource allocation	Advanced Design Techniques Hard
How does the FPTAS for knapsack work?	Fully Polynomial-Time Approximation Scheme: (1+ε)-approximation in poly(n,1/ε)<br>Scale and round item values to reduce number of distinct values<br>Run standard DP on rounded values<br>Error from rounding is bounded by ε fraction of optimal<br>Time: O(n³/ε), first FPTAS discovered	Advanced Design Techniques Hard
What is a persistent data structure and how is it implemented?	Preserves previous versions when modified (immutable)<br>Path copying: copy nodes on path from root to modified node<br>Fat node method: store all versions of each node<br>Applications: functional programming, version control, computational geometry<br>Space overhead typically O(log n) per update	Advanced Data Structures Hard
How does a splay tree achieve O(log n) amortized time?	Self-adjusting BST that moves accessed nodes toward root via splaying<br>Splaying: sequence of rotations (zig, zig-zig, zig-zag)<br>Frequently accessed nodes end up near root<br>Working set theorem: amortized cost depends on access locality<br>No worst-case guarantees but excellent practical performance	Advanced Data Structures Hard
What is the push-relabel algorithm for maximum flow?	Maintains preflow (flow into node ≥ flow out) and height function<br>Push: send flow along admissible edges (downhill)<br>Relabel: increase height when no admissible edges exist<br>Eventually converges to maximum flow<br>FIFO implementation: O(V³), highest-label: O(V²√E)	Network Flows Hard
How do you find edge-disjoint paths in a network?	Max number of edge-disjoint s-t paths equals min s-t cut capacity<br>Model as max flow with all edge capacities = 1<br>Each unit of flow corresponds to one edge-disjoint path<br>Vertex-disjoint paths: split each vertex (except s,t) into in and out vertices<br>Applications: fault tolerance, load balancing	Network Flows Medium
